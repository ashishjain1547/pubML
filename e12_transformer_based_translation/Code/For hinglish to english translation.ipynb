{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1725145f-dbb2-492b-86fd-0a567a8c03d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 16:17:19.565232: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-14 16:17:19.565267: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras_transformer import get_model, decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f7e79ec-e96d-494d-8af4-58e7588d3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/English_Hindi_Hinglish.txt', mode = 'r') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "data = data[0:55] # 55 Because we have that many labeled data points for Hinglish to English translation.\n",
    "\n",
    "source_tokens = [i.split(',')[1].strip().split(' ') for i in data]\n",
    "target_tokens = [i.split('\t')[0].strip().split(' ') for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f16a0fdf-fb5c-4b39-8656-32540b2e853b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Encoder-Input (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " Encoder-Token-Embedding (Embed  [(None, None, 32),  3168        ['Encoder-Input[0][0]']          \n",
      " dingRet)                        (99, 32)]                                                        \n",
      "                                                                                                  \n",
      " Encoder-Embedding (TrigPosEmbe  (None, None, 32)    0           ['Encoder-Token-Embedding[0][0]']\n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, None, 32)    4224        ['Encoder-Embedding[0][0]']      \n",
      " on (MultiHeadAttention)                                                                          \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, None, 32)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, None, 32)    0           ['Encoder-Embedding[0][0]',      \n",
      " on-Add (Add)                                                     'Encoder-1-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, None, 32)    64          ['Encoder-1-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward (FeedFor  (None, None, 32)    8352        ['Encoder-1-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Dropout   (None, None, 32)    0           ['Encoder-1-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Add (Add  (None, None, 32)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-1-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Norm (La  (None, None, 32)    64          ['Encoder-1-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, None, 32)    4224        ['Encoder-1-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Decoder-Input (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, None, 32)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Decoder-Token-Embedding (Embed  [(None, None, 32),  3168        ['Decoder-Input[0][0]']          \n",
      " dingRet)                        (99, 32)]                                                        \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, None, 32)    0           ['Encoder-1-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-2-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Decoder-Embedding (TrigPosEmbe  (None, None, 32)    0           ['Decoder-Token-Embedding[0][0]']\n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, None, 32)    64          ['Encoder-2-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadSelfAttenti  (None, None, 32)    4224        ['Decoder-Embedding[0][0]']      \n",
      " on (MultiHeadAttention)                                                                          \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward (FeedFor  (None, None, 32)    8352        ['Encoder-2-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadSelfAttenti  (None, None, 32)    0           ['Decoder-1-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Dropout   (None, None, 32)    0           ['Encoder-2-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadSelfAttenti  (None, None, 32)    0           ['Decoder-Embedding[0][0]',      \n",
      " on-Add (Add)                                                     'Decoder-1-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Add (Add  (None, None, 32)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-2-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadSelfAttenti  (None, None, 32)    64          ['Decoder-1-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Norm (La  (None, None, 32)    64          ['Encoder-2-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadQueryAttent  (None, None, 32)    4224        ['Decoder-1-MultiHeadSelfAttentio\n",
      " ion (MultiHeadAttention)                                        n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-2-FeedForward-Norm[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'Encoder-2-FeedForward-Norm[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadQueryAttent  (None, None, 32)    0           ['Decoder-1-MultiHeadQueryAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadQueryAttent  (None, None, 32)    0           ['Decoder-1-MultiHeadSelfAttentio\n",
      " ion-Add (Add)                                                   n-Norm[0][0]',                   \n",
      "                                                                  'Decoder-1-MultiHeadQueryAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadQueryAttent  (None, None, 32)    64          ['Decoder-1-MultiHeadQueryAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Decoder-1-FeedForward (FeedFor  (None, None, 32)    8352        ['Decoder-1-MultiHeadQueryAttenti\n",
      " ward)                                                           on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Decoder-1-FeedForward-Dropout   (None, None, 32)    0           ['Decoder-1-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Decoder-1-FeedForward-Add (Add  (None, None, 32)    0           ['Decoder-1-MultiHeadQueryAttenti\n",
      " )                                                               on-Norm[0][0]',                  \n",
      "                                                                  'Decoder-1-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Decoder-1-FeedForward-Norm (La  (None, None, 32)    64          ['Decoder-1-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadSelfAttenti  (None, None, 32)    4224        ['Decoder-1-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadSelfAttenti  (None, None, 32)    0           ['Decoder-2-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadSelfAttenti  (None, None, 32)    0           ['Decoder-1-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Decoder-2-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadSelfAttenti  (None, None, 32)    64          ['Decoder-2-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadQueryAttent  (None, None, 32)    4224        ['Decoder-2-MultiHeadSelfAttentio\n",
      " ion (MultiHeadAttention)                                        n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-2-FeedForward-Norm[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'Encoder-2-FeedForward-Norm[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadQueryAttent  (None, None, 32)    0           ['Decoder-2-MultiHeadQueryAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadQueryAttent  (None, None, 32)    0           ['Decoder-2-MultiHeadSelfAttentio\n",
      " ion-Add (Add)                                                   n-Norm[0][0]',                   \n",
      "                                                                  'Decoder-2-MultiHeadQueryAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadQueryAttent  (None, None, 32)    64          ['Decoder-2-MultiHeadQueryAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Decoder-2-FeedForward (FeedFor  (None, None, 32)    8352        ['Decoder-2-MultiHeadQueryAttenti\n",
      " ward)                                                           on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Decoder-2-FeedForward-Dropout   (None, None, 32)    0           ['Decoder-2-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Decoder-2-FeedForward-Add (Add  (None, None, 32)    0           ['Decoder-2-MultiHeadQueryAttenti\n",
      " )                                                               on-Norm[0][0]',                  \n",
      "                                                                  'Decoder-2-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Decoder-2-FeedForward-Norm (La  (None, None, 32)    64          ['Decoder-2-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Decoder-Output (EmbeddingSim)  (None, None, 99)     99          ['Decoder-2-FeedForward-Norm[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'Decoder-Token-Embedding[0][1]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 65,827\n",
      "Trainable params: 65,827\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Generate dictionaries\n",
    "def build_token_dict(token_list):\n",
    "    token_dict = {\n",
    "        '<PAD>': 0,\n",
    "        '<START>': 1,\n",
    "        '<END>': 2,\n",
    "    }\n",
    "    for tokens in token_list:\n",
    "        for token in tokens:\n",
    "            if token not in token_dict:\n",
    "                token_dict[token] = len(token_dict)\n",
    "    return token_dict\n",
    "\n",
    "source_token_dict = build_token_dict(source_tokens)\n",
    "target_token_dict = build_token_dict(target_tokens)\n",
    "target_token_dict_inv = {v: k for k, v in target_token_dict.items()}\n",
    "\n",
    "# Add special tokens\n",
    "encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "decode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens]\n",
    "output_tokens = [tokens + ['<END>', '<PAD>'] for tokens in target_tokens]\n",
    "\n",
    "# Padding\n",
    "source_max_len = max(map(len, encode_tokens))\n",
    "target_max_len = max(map(len, decode_tokens))\n",
    "\n",
    "encode_tokens = [tokens + ['<PAD>'] * (source_max_len - len(tokens)) for tokens in encode_tokens]\n",
    "decode_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in decode_tokens]\n",
    "output_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in output_tokens]\n",
    "\n",
    "encode_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in encode_tokens]\n",
    "decode_input = [list(map(lambda x: target_token_dict[x], tokens)) for tokens in decode_tokens]\n",
    "decode_output = [list(map(lambda x: [target_token_dict[x]], tokens)) for tokens in output_tokens]\n",
    "\n",
    "# Build & fit model\n",
    "model = get_model(\n",
    "    token_num=max(len(source_token_dict), len(target_token_dict)),\n",
    "    embed_dim=32,\n",
    "    encoder_num=2,\n",
    "    decoder_num=2,\n",
    "    head_num=4,\n",
    "    hidden_dim=128,\n",
    "    dropout_rate=0.05,\n",
    "    use_same_embed=False,  # Use different embeddings for different languages\n",
    ")\n",
    "model.compile('adam', 'sparse_categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6298a5e4-9e8c-4513-bc3e-149b7c1a7ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1760/1760 [==============================] - 46s 20ms/step - loss: 0.2883\n",
      "Epoch 2/10\n",
      "1760/1760 [==============================] - 35s 20ms/step - loss: 0.0089\n",
      "Epoch 3/10\n",
      "1760/1760 [==============================] - 34s 19ms/step - loss: 0.0059\n",
      "Epoch 4/10\n",
      "1760/1760 [==============================] - 33s 19ms/step - loss: 0.0054\n",
      "Epoch 5/10\n",
      "1760/1760 [==============================] - 33s 19ms/step - loss: 0.0075\n",
      "Epoch 6/10\n",
      "1760/1760 [==============================] - 33s 19ms/step - loss: 0.0054\n",
      "Epoch 7/10\n",
      "1760/1760 [==============================] - 33s 19ms/step - loss: 0.0052\n",
      "Epoch 8/10\n",
      "1760/1760 [==============================] - 33s 19ms/step - loss: 0.0052\n",
      "Epoch 9/10\n",
      "1760/1760 [==============================] - 33s 19ms/step - loss: 0.0071\n",
      "Epoch 10/10\n",
      "1760/1760 [==============================] - 33s 19ms/step - loss: 0.0056\n",
      "CPU times: user 11min 31s, sys: 56 s, total: 12min 27s\n",
      "Wall time: 5min 48s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8f347f69d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(\n",
    "    x=[np.array(encode_input * 1024), np.array(decode_input * 1024)],\n",
    "    y=np.array(decode_output * 1024),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1e46380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8f34c8b940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8f34c8b940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Wow!\n",
      "Help!\n",
      "Jump.\n",
      "Jump.\n",
      "Jump.\n",
      "Hello!\n",
      "Hello!\n",
      "Cheers!\n",
      "Cheers!\n",
      "Got it?\n",
      "I'm OK.\n",
      "Awesome!\n",
      "Come in.\n",
      "Get out!\n",
      "Go away!\n",
      "Goodbye!\n",
      "Perfect!\n",
      "Perfect!\n",
      "Welcome.\n",
      "Welcome.\n",
      "Have fun.\n",
      "Have fun.\n",
      "Have fun.\n",
      "I forgot.\n",
      "I forgot.\n",
      "I'll pay.\n",
      "I'm fine.\n",
      "I'm full.\n",
      "Let's go!\n",
      "Answer me.\n",
      "Birds fly.\n",
      "Excuse me.\n",
      "Fantastic!\n",
      "I fainted.\n",
      "I fear so.\n",
      "I laughed.\n",
      "I'm bored.\n",
      "I'm broke.\n",
      "I am tired.\n",
      "It's cold.\n",
      "Who knows?\n",
      "Who knows?\n",
      "Who knows?\n",
      "Who knows?\n",
      "Wonderful!\n",
      "Birds sing.\n",
      "Come on in.\n",
      "Definitely!\n",
      "Don't move.\n",
      "Fire burns.\n",
      "Follow him.\n",
      "I am tired.\n",
      "I can swim.\n",
      "I can swim.\n",
      "I love you.\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "decoded = decode(\n",
    "    model,\n",
    "    encode_input,\n",
    "    start_token=target_token_dict['<START>'],\n",
    "    end_token=target_token_dict['<END>'],\n",
    "    pad_token=target_token_dict['<PAD>'],\n",
    ")\n",
    "for i in decoded:\n",
    "    print(' '.join(map(lambda x: target_token_dict_inv[x], i[1:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb4bd5-1379-46cf-b5c8-138c20085777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be3596-52cd-4761-b2a9-595f54cb0df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd4e4ca-58f8-4715-902b-d872b59aa6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982e90c-3e54-4623-9595-c8a65639029a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
