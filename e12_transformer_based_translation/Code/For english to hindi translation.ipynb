{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1725145f-dbb2-492b-86fd-0a567a8c03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras_transformer import get_model, decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f7e79ec-e96d-494d-8af4-58e7588d3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/English_Hindi.txt', mode = 'r') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "source_tokens = [i.split(\"\t\")[0].split(' ') for i in data[0:10]]\n",
    "target_tokens = [i.split(\"\t\")[1].split(' ') for i in data[0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f16a0fdf-fb5c-4b39-8656-32540b2e853b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Encoder-Input (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " Encoder-Token-Embedding (Embed  [(None, None, 32),  480         ['Encoder-Input[0][0]']          \n",
      " dingRet)                        (15, 32)]                                                        \n",
      "                                                                                                  \n",
      " Encoder-Embedding (TrigPosEmbe  (None, None, 32)    0           ['Encoder-Token-Embedding[0][0]']\n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, None, 32)    4224        ['Encoder-Embedding[0][0]']      \n",
      " on (MultiHeadAttention)                                                                          \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, None, 32)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, None, 32)    0           ['Encoder-Embedding[0][0]',      \n",
      " on-Add (Add)                                                     'Encoder-1-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, None, 32)    64          ['Encoder-1-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward (FeedFor  (None, None, 32)    8352        ['Encoder-1-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Dropout   (None, None, 32)    0           ['Encoder-1-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Add (Add  (None, None, 32)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-1-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Norm (La  (None, None, 32)    64          ['Encoder-1-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, None, 32)    4224        ['Encoder-1-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Decoder-Input (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, None, 32)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Decoder-Token-Embedding (Embed  [(None, None, 32),  480         ['Decoder-Input[0][0]']          \n",
      " dingRet)                        (15, 32)]                                                        \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, None, 32)    0           ['Encoder-1-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-2-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Decoder-Embedding (TrigPosEmbe  (None, None, 32)    0           ['Decoder-Token-Embedding[0][0]']\n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, None, 32)    64          ['Encoder-2-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadSelfAttenti  (None, None, 32)    4224        ['Decoder-Embedding[0][0]']      \n",
      " on (MultiHeadAttention)                                                                          \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward (FeedFor  (None, None, 32)    8352        ['Encoder-2-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadSelfAttenti  (None, None, 32)    0           ['Decoder-1-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Dropout   (None, None, 32)    0           ['Encoder-2-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadSelfAttenti  (None, None, 32)    0           ['Decoder-Embedding[0][0]',      \n",
      " on-Add (Add)                                                     'Decoder-1-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Add (Add  (None, None, 32)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-2-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadSelfAttenti  (None, None, 32)    64          ['Decoder-1-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Norm (La  (None, None, 32)    64          ['Encoder-2-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadQueryAttent  (None, None, 32)    4224        ['Decoder-1-MultiHeadSelfAttentio\n",
      " ion (MultiHeadAttention)                                        n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-2-FeedForward-Norm[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'Encoder-2-FeedForward-Norm[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadQueryAttent  (None, None, 32)    0           ['Decoder-1-MultiHeadQueryAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadQueryAttent  (None, None, 32)    0           ['Decoder-1-MultiHeadSelfAttentio\n",
      " ion-Add (Add)                                                   n-Norm[0][0]',                   \n",
      "                                                                  'Decoder-1-MultiHeadQueryAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadQueryAttent  (None, None, 32)    64          ['Decoder-1-MultiHeadQueryAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Decoder-1-FeedForward (FeedFor  (None, None, 32)    8352        ['Decoder-1-MultiHeadQueryAttenti\n",
      " ward)                                                           on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Decoder-1-FeedForward-Dropout   (None, None, 32)    0           ['Decoder-1-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Decoder-1-FeedForward-Add (Add  (None, None, 32)    0           ['Decoder-1-MultiHeadQueryAttenti\n",
      " )                                                               on-Norm[0][0]',                  \n",
      "                                                                  'Decoder-1-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Decoder-1-FeedForward-Norm (La  (None, None, 32)    64          ['Decoder-1-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadSelfAttenti  (None, None, 32)    4224        ['Decoder-1-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadSelfAttenti  (None, None, 32)    0           ['Decoder-2-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadSelfAttenti  (None, None, 32)    0           ['Decoder-1-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Decoder-2-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadSelfAttenti  (None, None, 32)    64          ['Decoder-2-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadQueryAttent  (None, None, 32)    4224        ['Decoder-2-MultiHeadSelfAttentio\n",
      " ion (MultiHeadAttention)                                        n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-2-FeedForward-Norm[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'Encoder-2-FeedForward-Norm[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadQueryAttent  (None, None, 32)    0           ['Decoder-2-MultiHeadQueryAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadQueryAttent  (None, None, 32)    0           ['Decoder-2-MultiHeadSelfAttentio\n",
      " ion-Add (Add)                                                   n-Norm[0][0]',                   \n",
      "                                                                  'Decoder-2-MultiHeadQueryAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadQueryAttent  (None, None, 32)    64          ['Decoder-2-MultiHeadQueryAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Decoder-2-FeedForward (FeedFor  (None, None, 32)    8352        ['Decoder-2-MultiHeadQueryAttenti\n",
      " ward)                                                           on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Decoder-2-FeedForward-Dropout   (None, None, 32)    0           ['Decoder-2-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Decoder-2-FeedForward-Add (Add  (None, None, 32)    0           ['Decoder-2-MultiHeadQueryAttenti\n",
      " )                                                               on-Norm[0][0]',                  \n",
      "                                                                  'Decoder-2-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Decoder-2-FeedForward-Norm (La  (None, None, 32)    64          ['Decoder-2-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Decoder-Output (EmbeddingSim)  (None, None, 15)     15          ['Decoder-2-FeedForward-Norm[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'Decoder-Token-Embedding[0][1]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 60,367\n",
      "Trainable params: 60,367\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Generate dictionaries\n",
    "def build_token_dict(token_list):\n",
    "    token_dict = {\n",
    "        '<PAD>': 0,\n",
    "        '<START>': 1,\n",
    "        '<END>': 2,\n",
    "    }\n",
    "    for tokens in token_list:\n",
    "        for token in tokens:\n",
    "            if token not in token_dict:\n",
    "                token_dict[token] = len(token_dict)\n",
    "    return token_dict\n",
    "\n",
    "source_token_dict = build_token_dict(source_tokens)\n",
    "target_token_dict = build_token_dict(target_tokens)\n",
    "target_token_dict_inv = {v: k for k, v in target_token_dict.items()}\n",
    "\n",
    "# Add special tokens\n",
    "encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "decode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens]\n",
    "output_tokens = [tokens + ['<END>', '<PAD>'] for tokens in target_tokens]\n",
    "\n",
    "# Padding\n",
    "source_max_len = max(map(len, encode_tokens))\n",
    "target_max_len = max(map(len, decode_tokens))\n",
    "\n",
    "encode_tokens = [tokens + ['<PAD>'] * (source_max_len - len(tokens)) for tokens in encode_tokens]\n",
    "decode_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in decode_tokens]\n",
    "output_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in output_tokens]\n",
    "\n",
    "encode_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in encode_tokens]\n",
    "decode_input = [list(map(lambda x: target_token_dict[x], tokens)) for tokens in decode_tokens]\n",
    "decode_output = [list(map(lambda x: [target_token_dict[x]], tokens)) for tokens in output_tokens]\n",
    "\n",
    "# Build & fit model\n",
    "model = get_model(\n",
    "    token_num=max(len(source_token_dict), len(target_token_dict)),\n",
    "    embed_dim=32,\n",
    "    encoder_num=2,\n",
    "    decoder_num=2,\n",
    "    head_num=4,\n",
    "    hidden_dim=128,\n",
    "    dropout_rate=0.05,\n",
    "    use_same_embed=False,  # Use different embeddings for different languages\n",
    ")\n",
    "model.compile('adam', 'sparse_categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6298a5e4-9e8c-4513-bc3e-149b7c1a7ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "320/320 [==============================] - 16s 17ms/step - loss: 0.5669\n",
      "Epoch 2/10\n",
      "320/320 [==============================] - 5s 16ms/step - loss: 0.1369\n",
      "Epoch 3/10\n",
      "320/320 [==============================] - 5s 16ms/step - loss: 0.1262\n",
      "Epoch 4/10\n",
      "320/320 [==============================] - 5s 16ms/step - loss: 0.1244\n",
      "Epoch 5/10\n",
      "320/320 [==============================] - 5s 16ms/step - loss: 0.1236\n",
      "Epoch 6/10\n",
      "320/320 [==============================] - 5s 16ms/step - loss: 0.1232\n",
      "Epoch 7/10\n",
      "320/320 [==============================] - 5s 16ms/step - loss: 0.1226\n",
      "Epoch 8/10\n",
      "320/320 [==============================] - 5s 16ms/step - loss: 0.1226\n",
      "Epoch 9/10\n",
      "320/320 [==============================] - 5s 16ms/step - loss: 0.1224\n",
      "Epoch 10/10\n",
      "320/320 [==============================] - 5s 16ms/step - loss: 0.1224\n",
      "CPU times: user 1min 55s, sys: 8.97 s, total: 2min 4s\n",
      "Wall time: 1min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f253b9fe9a0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(\n",
    "    x=[np.array(encode_input * 1024), np.array(decode_input * 1024)],\n",
    "    y=np.array(decode_output * 1024),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1e46380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "वाह!\n",
      "\n",
      "बचाओ!\n",
      "\n",
      "छलांग.\n",
      "\n",
      "छलांग.\n",
      "\n",
      "छलांग.\n",
      "\n",
      "नमस्कार।\n",
      "\n",
      "नमस्कार।\n",
      "\n",
      "चियर्स!\n",
      "\n",
      "चियर्स!\n",
      "\n",
      "समझे कि नहीं?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "decoded = decode(\n",
    "    model,\n",
    "    encode_input,\n",
    "    start_token=target_token_dict['<START>'],\n",
    "    end_token=target_token_dict['<END>'],\n",
    "    pad_token=target_token_dict['<PAD>'],\n",
    ")\n",
    "for i in decoded:\n",
    "    print(' '.join(map(lambda x: target_token_dict_inv[x], i[1:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb4bd5-1379-46cf-b5c8-138c20085777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
